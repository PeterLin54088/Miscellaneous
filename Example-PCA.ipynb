{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preface**\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "In data analysis, gaining new insights often involves reinterpreting a dataset from different perspectives. One such approach is the *change of basis*, which transforms data into a new coordinate system. However, not all transformations are equally insightful. To identify a **meaningful** basis, we define specific criteria based on what we find valuable and interesting, seeking transformations that provide deeper insights into the data.\n",
    "\n",
    "#### **Change of Basis**\n",
    "\n",
    "A *change of basis* can be represented as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_{\\beta} = \\mathbf{X}_{\\text{std}} \\mathbf{P}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{X}_{\\text{std}}$ is the data in the **standard basis**.\n",
    "- $\\mathbf{X}_{\\beta}$ is the data in the new basis $\\beta$, defined by the transformation matrix $\\mathbf{P}$.\n",
    "\n",
    "The goal of this transformation is to find a basis that simplifies or clarifies the data, making its underlying structure more apparent.\n",
    "\n",
    "#### **Determining a Meaningful Basis**\n",
    "\n",
    "To identify a **meaningful** basis, we must define specific criteria based on the characteristics we want to highlight—whether it’s maximizing variance (as in PCA), maximizing correlation (as in CCA), or maximizing spread (as in MDS). We then search for a transformation matrix $\\mathbf{P}$ that optimizes the data according to these criteria.\n",
    "\n",
    "#### **Optimizing a Target Function**\n",
    "\n",
    "The search for a meaningful basis is typically driven by the goal of *optimizing a target function*—essentially an optimization problem. The transformation matrix $\\mathbf{P}$ is chosen by maximizing or minimizing a function $f_{\\mathbf{X}}(\\mathbf{P})$, which evaluates how well the transformation aligns with our defined criteria.\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\text{min} \\, f_{\\mathbf{X}}(\\mathbf{P}) \\quad \\text{or} \\quad \\text{arg} \\, \\text{max} \\, f_{\\mathbf{X}}(\\mathbf{P})\n",
    "$$\n",
    "\n",
    "Optimizing this function helps identify the optimal basis, offering the most insightful representation of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Principal Component Analysis (PCA)**\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "Principal Component Analysis (PCA) is one of the most common techniques for identifying a meaningful basis in a dataset. The specific criterion for PCA is to **maximize variance**, which is equivalent to minimizing reconstruction error.\n",
    "\n",
    "Also, in atmospheric science, PCA is also called **Empirical Orthogonal Projection (EOF)**.\n",
    "\n",
    "### **Dataset Representation**\n",
    "\n",
    "Consider a dataset matrix $\\mathbf{X}$ with dimensions $m \\times n$, where each row represents a sample and each column represents a feature (such as temperature, moisture, or vorticity). For simplicity, we assume that the columns of $\\mathbf{X}$ have been **centered**, meaning the mean of each feature is subtracted from its corresponding values. Thus, the matrix $\\mathbf{X}$ becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "- & \\mathbf{x}_1 & - \\\\\n",
    "- & \\mathbf{x}_2 & - \\\\\n",
    "- & \\mathbf{x}_3 & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & \\mathbf{x}_m & - \\\\\n",
    "\\end{bmatrix}_{m \\times n}\n",
    "$$\n",
    "\n",
    "Next, the covariance matrix $\\mathbf{C}$ is given by the eigen-decomposition:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{X}^T \\mathbf{X} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{Q} = \\left[\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_n \\right]$ is the matrix of eigenvectors of $\\mathbf{C}$.\n",
    "- $\\mathbf{\\Lambda} = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)$ is the diagonal matrix of eigenvalues of $\\mathbf{C}$, with $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n \\geq 0$.\n",
    "\n",
    "### **Criteria of PCA**\n",
    "\n",
    "The objective in PCA is to find an **orthonormal linear transformation** $\\mathbf{P} = [\\mathbf{p}_1, \\ldots, \\mathbf{p}_n]$ such that the variance in the transformed data (called principle components) $\\mathbf{X}_{\\text{std}} \\mathbf{P}$ is maximized along the first coordinate (the first principal component), the second greatest variance along the second coordinate, and so on.\n",
    "\n",
    "Mathematically, for the $k$-th principal component, we define the target function as the variance along the $k$-th coordinate:\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{\\hat{X}}_k}(\\mathbf{p}_k) = \\mathbf{p}_k^T  \\mathbf{\\hat{X}}_k^T \\mathbf{\\hat{X}}_k \\mathbf{p}_k = \\mathbf{p}_k^T  \\mathbf{\\hat{C}}_k \\mathbf{p}_k\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{\\hat{X}}_k = \\mathbf{X} - \\sum_{s=1}^{k-1} \\mathbf{X} \\mathbf{p}_s \\mathbf{p}_s^T$\n",
    "- $\\mathbf{\\hat{C}}_k = \\mathbf{\\hat{X}}_k^T \\mathbf{\\hat{X}}_k$\n",
    "\n",
    "To maximize the variance along each principal component, we seek to **maximize** the target function, leading to the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{p}_k \\Vert  = 1}{\\text{max}} \\, \\mathbf{p}_k^T  \\mathbf{\\hat{C}}_k \\mathbf{p}_k\n",
    "$$\n",
    "\n",
    "### **Finding the Transformation Matrix $\\mathbf{P}$**\n",
    "\n",
    "#### **First Direction ($k = 1$)**\n",
    "\n",
    "Consider the following optimization problem for the first principal component:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{p}_1 \\Vert = 1}{\\text{max}} \\, \\mathbf{p}_1^T \\mathbf{X}_1^T \\mathbf{X}_1 \\mathbf{p}_1\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{p}_1 \\Vert = 1}{\\text{max}} \\, \\mathbf{p}_1^T \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T \\mathbf{p}_1\n",
    "$$\n",
    "\n",
    "By defining a temporary variable $\\mathbf{w} = \\mathbf{Q}^T \\mathbf{p}_1$, where $\\Vert \\mathbf{w} \\Vert = 1$, the optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\text{max}} \\, \\mathbf{w}^T \\mathbf{\\Lambda} \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{\\Lambda}$ is a diagonal matrix, we can rewrite the target function as a summation. Let $\\mathbf{w} = \\left[w_1, w_2, \\ldots, w_n\\right]^T$, so the optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\text{max}} \\, \\mathbf{w}^T \\mathbf{\\Lambda} \\mathbf{w} = \\text{arg} \\, \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\text{max}} \\, \\sum_{i=1}^{n} w_i^2 \\lambda_i\n",
    "$$\n",
    "\n",
    "Given that $\\sum_{i=1}^{n} w_i^2 = 1$, the optimal solution (using the method of Lagrange multipliers) is as follows:\n",
    "\n",
    "- $w_i^2 = 1$ for the largest $\\lambda_i$ (which corresponds to $\\lambda_1$), and $w_i^2 = 0$ for all $i \\neq 1$.\n",
    "- This implies $\\mathbf{w} = \\left[1, 0, \\ldots, 0\\right]^T$.\n",
    "- Therefore, $\\mathbf{p}_1 = \\mathbf{q}_1$.\n",
    "\n",
    "Thus, the first principal component is the eigenvector corresponding to the largest eigenvalue.\n",
    "\n",
    "#### **Subsequent Directions ($k = 2, \\ldots, n$)**\n",
    "\n",
    "For the $k$-th principal component, the optimization problem is:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{p}_k \\Vert = 1}{\\text{max}} \\, \\mathbf{p}_k^T \\mathbf{\\hat{X}}_k^T \\mathbf{\\hat{X}}_k \\mathbf{p}_k\n",
    "$$\n",
    "\n",
    "The matrix $\\mathbf{\\hat{X}}_k^T \\mathbf{\\hat{X}}_k$ can be expressed as (the proof is somewhat lengthy, so we leave it in the appendix):\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{X}}_k^T \\mathbf{\\hat{X}}_k = \\mathbf{Q} \\mathbf{\\hat{\\Lambda}}_k \\mathbf{Q}^T, \\quad \\text{where} \\quad \\mathbf{\\hat{\\Lambda}}_k = \\text{diag}\\left( 0, \\ldots, 0, \\lambda_k, \\ldots, \\lambda_n \\right)\n",
    "$$\n",
    "\n",
    "Thus, the optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{p}_k \\Vert = 1}{\\text{max}} \\, \\mathbf{p}_k^T \\mathbf{Q} \\mathbf{\\hat{\\Lambda}}_k \\mathbf{Q}^T \\mathbf{p}_k\n",
    "$$\n",
    "\n",
    "Again, we define a temporary variable $\\mathbf{w} = \\mathbf{Q}^T \\mathbf{p}_k$, where $\\Vert \\mathbf{w} \\Vert = 1$. This transforms the problem into:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\text{max}} \\, \\mathbf{w}^T \\mathbf{\\hat{\\Lambda}}_k \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{\\hat{\\Lambda}}_k$ is a diagonal matrix, we can rewrite the target function as a summation. Let $\\mathbf{w} = \\left[w_1, w_2, \\ldots, w_n\\right]^T$, so the optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\text{max}} \\, \\mathbf{w}^T \\mathbf{\\hat{\\Lambda}}_k \\mathbf{w} = \\text{arg} \\, \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\text{max}} \\, \\sum_{i=1}^{n} w_i^2 \\lambda_i\n",
    "$$\n",
    "\n",
    "Given that $\\sum_{i=1}^{n} w_i^2 = 1$, the optimal solution (using the method of Lagrange multipliers) is as follows:\n",
    "\n",
    "- $w_i^2 = 1$ for the largest $\\lambda_i$ (which corresponds to $\\lambda_k$), and $w_i^2 = 0$ for all $i \\neq k$.\n",
    "- This implies $\\mathbf{w} = \\left[0, \\ldots, 1, \\ldots, 0\\right]^T$.\n",
    "- Therefore, $\\mathbf{p}_k = \\mathbf{q}_k$.\n",
    "\n",
    "Thus, the $k$-th principal component is the eigenvector corresponding to the $k$-th largest eigenvalue.\n",
    "\n",
    "### **Principal Components and Explained Variance**\n",
    "\n",
    "The scores for the $k^{\\text{th}}$ principal component are given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_k = \\mathbf{X} \\mathbf{p}_k\n",
    "$$\n",
    "\n",
    "In matrix form, the full set of principal component scores is:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X} \\mathbf{P}\n",
    "$$\n",
    "\n",
    "Using Singular Value Decomposition (SVD), we express $\\mathbf{X}$ as $\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$, where $\\mathbf{P} = \\mathbf{V}$ and $\\mathbf{\\Sigma}^2 = \\mathbf{\\Lambda}$. Therefore:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X} \\mathbf{V} = \\mathbf{U} \\mathbf{\\Sigma}\n",
    "$$\n",
    "\n",
    "The variance of the principal component scores is:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}^T \\mathbf{Y} = \\mathbf{\\Lambda} = \\mathbf{\\Sigma}^2\n",
    "$$\n",
    "\n",
    "The explained variance of the $k$-th principal component is:\n",
    "\n",
    "$$\n",
    "\\text{EV}(k) = \\frac{\\lambda_k}{\\sum_{i = 1}^{n} \\lambda_i} = \\frac{\\sigma_k^2}{\\sum_{i = 1}^{n} \\sigma_i^2}\n",
    "$$\n",
    "\n",
    "### **An Alternative View of PCA**\n",
    "\n",
    "PCA can also be interpreted as an optimization problem that maximizes variance while preserving as much information as possible. In this view, we aim to find a **low-rank** approximation of the data that **minimizes the reconstruction error**. This leads to the optimization:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\text{min} \\, \\Vert \\mathbf{X} - \\mathbf{X}\\mathbf{P}\\mathbf{P}^T \\Vert^2\n",
    "$$\n",
    "\n",
    "Above optimization problem will also lead to maximize variance, just as we've solved before.\n",
    "\n",
    "The optimal $r$-dimensional linear transformation is achieved by computing the best rank-$r$ approximation of $\\mathbf{X}$ using the L2 norm:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_r = \\mathbf{V}_r\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_r = \\mathbf{U}_r \\mathbf{\\hat{\\Sigma}}_r \\mathbf{V}_r^T\n",
    "$$\n",
    "\n",
    "Where\n",
    "- $\\mathbf{U}_r$ contain the first $r$ columns of $\\mathbf{U}$,\n",
    "- $\\mathbf{V}_r$ contain the first $r$ columns of $\\mathbf{V}$,\n",
    "- $\\mathbf{\\hat{\\Sigma}}_r$ contain the first $r$ largest singular value of $\\mathbf{\\Sigma}$,\n",
    "- $\\mathbf{X}_r$ is the reconstructed dataset,\n",
    "- $\\mathbf{P}_r$ is the best $r$-dimensional fit, identical to $\\mathbf{V}_r$.\n",
    "\n",
    "> For further reading, refer to [this source](https://rich-d-wilkinson.github.io/MATH3030/4.3-an-alternative-view-of-pca.html)\n",
    "\n",
    "---\n",
    "\n",
    "### **Appendix**\n",
    "\n",
    "#### **Centering the Data**\n",
    "\n",
    "PCA assumes that the data is centered (i.e., each feature has zero mean). If the data is not centered, the covariance matrix may be biased:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma} = n \\mathbf{\\mu}^T \\mathbf{\\mu} + \\frac{1}{n} \\mathbf{X}_{\\text{centered}}^T \\mathbf{X}_{\\text{centered}}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{\\mu}$ is the mean vector of the data. Always center the data before performing PCA to avoid distorting the covariance structure.\n",
    "\n",
    "> **Tip**: Always center your data before performing PCA!\n",
    "\n",
    "#### **Scaling of Features**\n",
    "\n",
    "PCA is sensitive to the variance of each feature. If some features are scaled differently, they can dominate the principal components. For example, scaling the columns of $\\mathbf{X}$ by a diagonal matrix $\\mathbf{S}$ alters the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T \\mathbf{X} \\Longrightarrow \\mathbf{S} \\mathbf{X}^T \\mathbf{X} \\mathbf{S} \\Longrightarrow \\mathbf{Q} \\mathbf{S} \\mathbf{\\Lambda} \\mathbf{S} \\mathbf{Q}^T \n",
    "$$\n",
    "\n",
    "Although this does not affect the orthonormal basis, but it does affect how we evaluate the magnitude of principle components and explained variance! This could happen when data included some information on units, like time in seconds or in days, distances in meter or kilometers.\n",
    "\n",
    "Therefore, be mindful of feature scaling when applying PCA, especially when features have different units or scales.\n",
    "\n",
    "> **Tip**: Ensure consistent feature scaling when magnitude matters, especially for variables with different units!\n",
    "\n",
    "#### **Rotated EOF**\n",
    "\n",
    "We've known that  is just an nickname for PCA, then, what is Rotated EOF (REOF)? In short, REOF is just doing PCA  then  rotate (specifically, orthogonal transformations) its eigenvectors.\n",
    "\n",
    "In PCA we only have to do SVD to find principal components and eigenvectors\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n",
    "$$\n",
    "\n",
    "Given an orthonormal linear transformation $\\mathbf{R}$ as the rotation matrix, we have \n",
    "\n",
    "$$\n",
    "\\mathbf{R}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{R} = \\mathbf{R}^T \\mathbf{V} \\mathbf{\\Sigma}^2 \\mathbf{V}^T \\mathbf{R}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{V}$ and $\\mathbf{R}^T$ are orthonormal, the principal components are identical. We say that the principal components are invariant under orthogonal transformations.\n",
    "\n",
    "In summary, REOF is just rotate eigenvectors, and sometimes might help providing physically meaningful informations.\n",
    "\n",
    "\n",
    "#### **Identity, $\\mathbf{\\hat{X}}^T_k \\mathbf{\\hat{X}}_k = \\mathbf{Q} \\mathbf{\\hat{\\Lambda}}_k \\mathbf{Q}^T$**\n",
    "\n",
    "The derivation for $\\mathbf{\\hat{X}}_k$ can be found in the appendix. Expanding the terms leads to the identity:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{X}}^T_k \\mathbf{\\hat{X}}_k = \\left( \\mathbf{X} - \\sum_{s=1}^{k-1} \\mathbf{X} \\mathbf{p}_s \\mathbf{p}_s^T \\right)^T \\left( \\mathbf{X} - \\sum_{s=1}^{k-1} \\mathbf{X} \\mathbf{p}_s \\mathbf{p}_s^T \\right)\n",
    "$$\n",
    "\n",
    "After canceling the terms, the final result is:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{X}}^T_k \\mathbf{\\hat{X}}_k = \\mathbf{Q} \\mathbf{\\hat{\\Lambda}}_k \\mathbf{Q}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Check if X^T @ X equals Q @ A @ Q^T: True\n",
      "Check if X^T @ X @ q_i equals λ_i * q_i for the first 21 eigenvectors: True\n",
      "====================================================================================================\n",
      "Check if X @ v_i equals σ_i * u_i for the first 11 singular vectors: True\n",
      "Check if X @ V equals U * Σ: True\n",
      "Check if X_raw @ V equals U * Σ + μ_X @ V: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dataset_generator(m, n):\n",
    "    \"\"\"\n",
    "    Let dataset matrix is mxn where m is the sample number and n is the feature number\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 1, m, endpoint=False)\n",
    "    x = np.linspace(0, 1, n, endpoint=True)\n",
    "    dataset = (np.exp(1j * 2 * np.pi * 5 * t).real + t).reshape(-1, 1) @ np.exp(1j * 2 * np.pi * 3 * x).real.reshape(1, -1)\n",
    "    dataset += np.random.normal(size=dataset.shape) * 2\n",
    "    dataset_centered = dataset - np.mean(dataset, axis=0, keepdims=True)\n",
    "    covariance = dataset_centered.T @ dataset_centered\n",
    "    return dataset, dataset_centered, covariance\n",
    "\n",
    "print(\"==========\"*10)\n",
    "\n",
    "m = 20\n",
    "n = 40\n",
    "r = n\n",
    "\n",
    "dataset, dataset_centered, covariance = dataset_generator(m, n)\n",
    "\n",
    "# Perform Eigenvalue Decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(covariance)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors in descending order\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Mask invalid eigenvalues and eigenvectors (optional)\n",
    "# If you comment below code, the results will still be **numerically** correct\n",
    "# Since np.linalg.eig(A) cannot handle rank-deficient matrix, small numerical errors undermine the eigenvectors!\n",
    "mask = np.abs(eigenvalues) < 1e-8\n",
    "eigenvalues[mask] = 0\n",
    "eigenvectors[:, mask] = 0\n",
    "\n",
    "# Check if the covariance matrix equals Q @ A @ Q^T (eigen-decomposition)\n",
    "print(f\"Check if X^T @ X equals Q @ A @ Q^T: {np.allclose(covariance, eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T)}\")\n",
    "\n",
    "# Select a random subset of eigenvectors (s)\n",
    "s = np.random.randint(1, r)\n",
    "\n",
    "# Check if X^T @ X @ q_i equals λ_i * q_i for the selected eigenvectors\n",
    "print(f\"Check if X^T @ X @ q_i equals λ_i * q_i for the first {s} eigenvectors: {np.allclose(covariance @ eigenvectors[:, :s], eigenvectors[:, :s] @ np.diag(eigenvalues[:s]))}\")\n",
    "\n",
    "\n",
    "print(\"==========\"*10)\n",
    "\n",
    "m = 20\n",
    "n = 40\n",
    "r = np.min([m, n])\n",
    "\n",
    "# Perform Singular Value Decomposition\n",
    "U, singular_values, VT = np.linalg.svd(dataset_centered, full_matrices=False)\n",
    "V = VT.T\n",
    "\n",
    "# Select a random subset of singular values\n",
    "s = np.random.randint(10, 20)\n",
    "\n",
    "# Check if X @ v_i equals σ_i * u_i for the selected subset\n",
    "print(f\"Check if X @ v_i equals σ_i * u_i for the first {s} singular vectors: \"\n",
    "      f\"{np.allclose(dataset_centered @ V[:, :s], U[:, :s] @ np.diag(singular_values[:s]))}\")\n",
    "\n",
    "# Check if X @ V equals U * Σ (Singular Value Decomposition consistency check)\n",
    "print(f\"Check if X @ V equals U * Σ: \"\n",
    "      f\"{np.allclose(dataset_centered @ V, U @ np.diag(singular_values))}\")\n",
    "\n",
    "# Check if X_raw @ V equals U * Σ + μ_X @ V (Including the mean of X)\n",
    "print(f\"Check if X_raw @ V equals U * Σ + μ_X @ V: \"\n",
    "      f\"{np.allclose(dataset @ V, U @ np.diag(singular_values) + np.ones(m).reshape(-1, 1) @ np.mean(dataset, axis=0).reshape(1, -1) @ V)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "__MAIN__",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
